{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Môn: Khai thác dữ liệu nâng cao\n",
    "giảng viên: T.S Nguyễn Ngọc Thảo\n",
    "Sinh viên: Trương Ngọc Tài\n",
    "Mssv: 1512473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dòng sản phẩm: dây đeo thay thế dùng cho 'Apple watch kích thước 38mm/42mm': \n",
    "<img src=\"apple_42_38.jpg\" width = \"150\" height = \"150\">\n",
    "\n",
    "\n",
    "\n",
    "Danh sách các sản phẩm:\n",
    "- [Asin: B07CL119KV] iGK Sport Band Compatible for Apple Watch 42mm 38mm, Soft Silicone Sport Strap Replacement Bands Compatible for iWatch Apple Watch Series 3, Series 2, Series 1 S/M M/L <img src=\".\\productPicture\\iGK.png\" width = \"150\" height = \"150\">\n",
    "- [Asin: B077VLBBYT] Yunsea Compatible for Apple Watch Band 38mm 42mm, Soft Nylon Sport Loop, with Hook and Loop Fastener, Replacement Band Compatible for iWatch Series 1/2/3 <img src=\".\\productPicture\\yunsea.png\" width = \"150\" height = \"150\">\n",
    "- [Asin: B0786QKGX3] VATI Replacement Band Compatible for Apple Watch Band 38mm 42mm Soft Breathable Nylon Sport Loop Band Adjustable Wrist Strap Replacement Band Compatible for iWatch Series 3/2/1,Sport,Nike+,Edition <img src=\".\\productPicture\\vati.png\" width = \"150\" height = \"150\">\n",
    "- [Asin: B07BQWGVDX] OULEDI Compatible Stainless Steel Band for Apple Watch Replacement Mesh Strap Bracelet for iWatch Series 1 Series 2 Series 3 Series 4 with Magnetic Closure Clasp 38mm 40mm Rose Gold <img src=\".\\productPicture\\ouledi.png\" width = \"150\" height = \"150\">\n",
    "- [Asin: B07GTG8FM3] iYou Sport Band Compatible for Apple Watch Band 38MM 42MM, Soft Silicone Replacement Sport Strap Compatible for iWatch 2017 Apple Watch Series 3/2/1, Edition, Nike+, All Models More Colors Choose <img src=\".\\productPicture\\iyou.png\" width = \"150\" height = \"150\">\n",
    "- [Asin: B075R4NNPH] Waterproof Apple Watch Case 38mm Series 3 & 2 with Premium Soft Silicone Apple Watch Band by Catalyst, Shock Proof Impact Resistant (not Compatible with The 42mm iWatch) <img src=\".\\productPicture\\Waterproof.png\" width = \"150\" height = \"150\">\n",
    "- [Asin: B079NJY3QL] Catalyst Apple Watch Case 38mm Series 3 & Series 2 Drop Proof Shock Proof Impact Protection Apple Watch case [Rugged iWatch Protective case], Army Green <img src=\".\\productPicture\\Catalyst.png\" width = \"150\" height = \"150\">\n",
    "- [Asin: B077NCMCR6] For Apple Watch Band 38mm, Maxjoy Nylon iWatch Strap Replacement Bands with Stainless Metal Clasp for Apple Watch Series 3 Series 2 Series 1 Sport and Edition, Army Green <img src=\".\\productPicture\\Maxjoy.png\" width = \"150\" height = \"150\">\n",
    "- [Asin: B071FK7GS6] Compatible Apple Watch Band 38mm Case, Camyse Shockproof Rugged Protective Cover with Bands Stainless Steel Clasp for iWatch Apple Watch Series 3, 2, 1 Sport Edition for Men Women grils boys - Black <img src=\".\\productPicture\\Camyse.png\" width = \"150\" height = \"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quy trình\n",
    "\n",
    "### 1/ Thu thập dữ liệu và phân chia dữ liệu\n",
    "### 2/ Xử lí dữ liệu\n",
    "### 3/ Xây dựng model\n",
    "### 4/ Huấn luyện model\n",
    "### 5/ Đánh giá model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# các thư viện sử dụng\n",
    "#from lxml import html  \n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import inflect\n",
    "\n",
    "#vẽ dữ liệu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser as dateparser\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#remove stop word\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "# stemming\n",
    "# PorterStemmer để steming\n",
    "# WordNetLematizer để chuyển đổi từ loại\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# tranform vector su dugn tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# model training\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# download data of nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('sentiwordnet')\n",
    "## n - NOUN \n",
    "## v - VERB \n",
    "## a - ADJECTIVE \n",
    "## s - ADJECTIVE SATELLITE \n",
    "## r - ADVERB \n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hàm parseReviews \n",
    "- tham số đầu vào: mã asin của sản phẩm\n",
    "- trả về là danh sách tất cả các reviews cho sản phẩm đó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseReviews(asin):\n",
    "    #This script has only been tested with Amazon.com\n",
    "    amazon_url  = 'http://www.amazon.com/product-reviews/'+asin\n",
    "    # Add some recent user agent to prevent amazon from blocking the request \n",
    "    # Find some chrome user agent strings  here https://udger.com/resources/ua-list/browser-detail?browser=Chrome\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'}\n",
    "\n",
    "    for i in range(3):\n",
    "\n",
    "        response = requests.get(amazon_url,headers = headers,verify=False)\n",
    "        if response.status_code==404:\n",
    "            return {\"url\":amazon_url,\"error\":\"page not found\"}\n",
    "        if response.status_code!=200:\n",
    "            continue\n",
    "\n",
    "        '''\n",
    "        The HTML xpath parser from lxml.etree seems to have max depth limit. \n",
    "        It won't traverse further to parse the text if the depth exceeds 254. \n",
    "        To avoid this, splitting the html into chunks\n",
    "        '''\n",
    "        # chuyển thành cây\n",
    "        tree = html.fromstring(response.text)\n",
    "        # gán xpath\n",
    "        xpath_pageNext = '//li[@data-reftag=\"cm_cr_arp_d_paging_btm\"]'\n",
    "\n",
    "        # tiến hành lấy thông tin link các trang\n",
    "        ls_page = tree.xpath(xpath_pageNext)\n",
    "        \n",
    "        ## lấy được các trang rồi thì break ngắt vong lập\n",
    "        break\n",
    "    \n",
    "    # lấy tất cả các link trang chứa reviews\n",
    "    # links\n",
    "    links = []\n",
    "    # lấ số lượng trang reviews\n",
    "    s = ls_page[4].find('a').get('href')\n",
    "    num_page = int(ls_page[4].find('a').get('href').split('&pageNumber=')[1])\n",
    "    for i in range(2, num_page + 1):\n",
    "        url = 'https://amazon.com/' + re.sub(r'ref=cm_cr_arp_d_paging_btm_.*?ie=UTF8&pageNumber=.*$', 'ref=cm_cr_arp_d_paging_btm_' + str(i) + '?ie=UTF8&pageNumber=' +  str(i), s)\n",
    "        links.append(url)\n",
    "    \n",
    "    # lấy reviews ở trang hiện tại\n",
    "    xpath_reviewsList = '//div[@id=\"cm_cr-review_list\"]'\n",
    "    xpath_review = '//div[@data-hook=\"review\"]'\n",
    "    \n",
    "    review_str = tree.xpath(xpath_reviewsList)\n",
    "    \n",
    "    for i in review_str:\n",
    "        # lấy từng chuỗi review\n",
    "        reviews = i.xpath(xpath_review)\n",
    "    \n",
    "    ## cấu trúc lưu dữ liệu là 1 dic.\n",
    "    reviews_list = []\n",
    "    \n",
    "    ## parse từng chuỗi review:\n",
    "    for review in reviews:\n",
    "        \n",
    "        ls_div = review.find('div').findall('div')\n",
    "        #### chỉ lấy 4 div đầu tiên theo thứ tự bên dưới\n",
    "        ### lấy tên\n",
    "        if ls_div[0].find('a') is None:\n",
    "                continue\n",
    "        name = ls_div[0].find('a').findall('div', {'class': 'a-profile-content'})#.findall('div')\n",
    "        name1 = name[1].find('span', {'class': 'a-profile-avatar-wrapper'}).text\n",
    "        \n",
    "        ### đầu tiên là lấy số sao\n",
    "        star = ls_div[1].find('a').find('i').find('span').text.replace(' out of 5 stars', '')\n",
    "        ### tiếp theo lấy thời gian\n",
    "        date = review.find('div').find('span').text\n",
    "        ### tiếp theo lấy text\n",
    "        review_content = ls_div[3].find('span').text\n",
    "        \n",
    "        ### tiến hành lưu dữ liệu ### tất cả review được chuyển về lower case\n",
    "        if review_content is not None:\n",
    "            dic_review = {\n",
    "                'review_author' : name1,\n",
    "                'review_rating' : star,\n",
    "                'review_posted_date' : date,\n",
    "                'review_text' : review_content.lower()\n",
    "            }\n",
    "            reviews_list.append(dic_review)\n",
    "        \n",
    "\n",
    "    # lấy reviews trong các trang còn lại\n",
    "    for link in links:\n",
    "        response = requests.get(link,headers = headers,verify=False)\n",
    "        \n",
    "        # chuyển thành cây\n",
    "        tree = html.fromstring(response.text)\n",
    "        \n",
    "        # tiến hành lấy dữ liệu như trên\n",
    "        review_str = tree.xpath(xpath_reviewsList)\n",
    "    \n",
    "        for i in review_str:\n",
    "            # lấy từng chuỗi review\n",
    "            reviews = i.xpath(xpath_review)\n",
    "        \n",
    "        ## parse từng chuỗi review:\n",
    "        for review in reviews:\n",
    "\n",
    "            ls_div = review.find('div').findall('div')\n",
    "            #### chỉ lấy 4 div đầu tiên theo thứ tự bên dưới\n",
    "            ### lấy tên\n",
    "            if ls_div[0].find('a') is None:\n",
    "                continue\n",
    "            name = ls_div[0].find('a').findall('div', {'class': 'a-profile-content'})#.findall('div')\n",
    "            name1 = name[1].find('span', {'class': 'a-profile-avatar-wrapper'}).text\n",
    "\n",
    "            ### đầu tiên là lấy số sao\n",
    "            star = ls_div[1].find('a').find('i').find('span').text.replace(' out of 5 stars', '')\n",
    "            ### tiếp theo lấy thời gian\n",
    "            date = review.find('div').find('span').text\n",
    "            ### tiếp theo lấy text\n",
    "            review_content = ls_div[3].find('span').text\n",
    "            \n",
    "            ### tiến hành lưu dữ liệu ### tất cả review được chuyển về lower case\n",
    "            if review_content is not None:\n",
    "                dic_review = {\n",
    "                    'review_author' : name1,\n",
    "                    'review_rating' : star,\n",
    "                    'review_posted_date' : date,\n",
    "                    'review_text' : review_content.lower()\n",
    "                }\n",
    "                reviews_list.append(dic_review)\n",
    "            \n",
    "    return reviews_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hàm ReadAsin\n",
    "- tham số đầu vào là tên file lưu các review\n",
    "- thực hiện đọc sanh sách mã Asin của từng sản phẩm truyền vào sau đó tiến hành lấy tất cả các reviews ứng với mã asin đó và lưu vào file input dưới dạng chuỗi json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadAsin(input):\n",
    "    AsinList = ['B01MS9O4JC', 'B015WKAQ1K', 'B07C44LMVQ', 'B07CSYNZMG', 'B07C68N8F4', 'B018ONN290', 'B01B61NLU2', 'B01NAWBGQH', 'B01AIH23QS', 'B01M2YQ73M', 'B07B9SLGLK', 'B075R4NNPH', 'B077NCMCR6', 'B071FK7GS6', 'B07CL119KV', 'B077VLBBYT', 'B0786QKGX3', 'B07BQWGVDX', 'B07C44LMVP']\n",
    "    extracted_data = []\n",
    "    for asin in AsinList:\n",
    "        print(\"Downloading and processing page http://www.amazon.com/product-reviews/\"+asin)\n",
    "        reviews = ParseReviews(asin)\n",
    "        reviews_len = len(reviews)\n",
    "        dic = {\n",
    "            'asin' : asin,\n",
    "            'reviews count' : reviews_len,\n",
    "            'reviews' : reviews\n",
    "        }\n",
    "        extracted_data.append(dic)\n",
    "        sleep(5)\n",
    "    with open(input, \"w\", encoding=\"utf-8\") as f:\n",
    "    #   f.write(extracted_data)\n",
    "    #f = open('DuLieuTho.json','w')\n",
    "        json.dump(extracted_data,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(review):\n",
    "    # khởi tạo 1 set stop word\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    # tách câu thành tokens  \n",
    "    word_tokens = word_tokenize(review)\n",
    "    filtered_sentence = word_tokens\n",
    "    # tra từng tokens trong set stopword và lưu phần còn lại vào lại câu\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    # tiến hành stemming từng từ trong câu\n",
    "    #filtered_sentence = [(WordNetLemmatizer().lemmatize(filtered_sentence[i],pos = 'v') if WordNetLemmatizer().lemmatize(filtered_sentence[i]).endswith('e' or 's' or 'ss' or 'sess') else PorterStemmer().stem(filtered_sentence[i])) for i in range(len(filtered_sentence))]\n",
    "    #filtered_sentence = [PorterStemmer().stem(filtered_sentence[i]) for i in range(len(filtered_sentence))]\n",
    "    filtered_sentence = [WordNetLemmatizer().lemmatize(filtered_sentence[i],pos = 'v') for i in range(len(filtered_sentence))]\n",
    "    # tiến hành convert số thành chữ tương ứng với số\n",
    "    filtered_sentence = [(inflect.engine().number_to_words(filtered_sentence[i]) if filtered_sentence[i].isdigit() else filtered_sentence[i]) for i in range(len(filtered_sentence))]\n",
    "    # trả về\n",
    "    re_review = ' '.join(filtered_sentence)\n",
    "    return re_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcReviews( input, outputJson):\n",
    "    # đọc file và load dữ liệu\n",
    "    f = open(input,'r')\n",
    "    data_jsons = json.loads(f.read())\n",
    "    f.close()\n",
    "    \n",
    "    num_pd = len(data_jsons)\n",
    "    for i_pd in range(num_pd):\n",
    "        num_reviews = len((data_jsons[i_pd])['reviews'])\n",
    "        for i_rv in range(num_reviews):\n",
    "            # chuyển start thành số\n",
    "            data_jsons[i_pd]['reviews'][i_rv]['review_rating'] = float(data_jsons[i_pd]['reviews'][i_rv]['review_rating'])\n",
    "            '''\n",
    "            # tiến hành tách câu sử dụng pickle của nltk\n",
    "            data_jsons[i_pd]['reviews'][i_rv]['review_text'] = tokenizer.tokenize(data_jsons[i_pd]['reviews'][i_rv]['review_text'], realign_boundaries=True)\n",
    "            #tiến hành tách từ và loại bỏ stopwords trong mỗi câu đồng thời stemming trong từng câu\n",
    "            num_sentences = len(data_jsons[i_pd]['reviews'][i_rv]['review_text'])\n",
    "            for sentence in range(num_sentences):\n",
    "                data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence] = removeStopWords(data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence])\n",
    "                # tiến hành loại bỏ dấu câu\n",
    "                data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence] = re.sub(r'[^a-zA-Z0-9 ]',r'',data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence])\n",
    "                # tiến hành loại bỏ tất cả các khoảng trắng dư thừa trong câu\n",
    "                data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence] = (re.sub(' +', ' ', data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence])).strip()\n",
    "                raw = ''.join(data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence])\n",
    "            '''\n",
    "            # remove stop word\n",
    "            data_jsons[i_pd]['reviews'][i_rv]['review_text'] = removeStopWords(data_jsons[i_pd]['reviews'][i_rv]['review_text'])\n",
    "            # loại bỏ dấu câu\n",
    "            data_jsons[i_pd]['reviews'][i_rv]['review_text'] = re.sub(r'[^a-zA-Z0-9 ]',r'',data_jsons[i_pd]['reviews'][i_rv]['review_text'])\n",
    "            #loại bỏ khoảng trắng dư thừa\n",
    "            data_jsons[i_pd]['reviews'][i_rv]['review_text'] = (re.sub(' +', ' ', data_jsons[i_pd]['reviews'][i_rv]['review_text'])).strip()\n",
    "            \n",
    "            \n",
    "    # tiến hành lưu lại dữ liệu\n",
    "    with open(outputJson, \"w\", encoding=\"utf-8\") as fJson:\n",
    "    #    f.write(data_jsons)\n",
    "    #f = open(output,'w')\n",
    "        json.dump(data_jsons,fJson,indent=4)\n",
    "        \n",
    "    print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcData (input,output):\n",
    "    f = open(input,'r',encoding=\"utf-8\")\n",
    "    f_data = f.readlines()\n",
    "    fout = open(output, 'w',encoding=\"utf-8\")\n",
    "    \n",
    "    # tiến hành xử lí\n",
    "    for rev in f_data:\n",
    "        review = removeStopWords(rev)\n",
    "        review = re.sub(r'[^a-zA-Z0-9 ]',r'',review)\n",
    "        review = (re.sub(' +', ' ', review)).strip()\n",
    "        # ghi vào file đã xử lí\n",
    "        fout.write(review + '\\n')\n",
    "    \n",
    "    f.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xử lí tay\n",
    "\n",
    "Giai đoạn xử lí tay ta chỉ xử lí trên tập train, không được phép xử lí trên tâp test.\n",
    "\n",
    "---\n",
    "\n",
    "B các kí tự: s, ve, ca, t, 38mm, 42mm, d.\n",
    "\n",
    "Tay thế: don = dont, doesn = doesnt, didn = didnt, isn = inst, aren = arent, 2nd, 3rd, 6th, ...\n",
    "\n",
    "Cỉnh sữa: soooo.., toooooo, \n",
    "\n",
    "B câu tiếng bla bla @@: buenas noches este artculo ha llegado mis manos enviaron una cosa por otra, pierde el brillo de inmediato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devideData (inputJson, pos_txt, neg_txt, neur_txt):\n",
    "    # đọc file và load dữ liệu\n",
    "    f = open(inputJson,'r',encoding=\"utf-8\")\n",
    "    data_jsons = json.loads(f.read())\n",
    "    f.close()\n",
    "    \n",
    "    # mở file text\n",
    "    f_pos = open(pos_txt, 'w', encoding='utf-8')\n",
    "    f_neg = open(neg_txt, 'w', encoding='utf-8')\n",
    "    f_neur = open(neur_txt, 'w', encoding='utf-8')\n",
    "    \n",
    "    num_pd = len(data_jsons)\n",
    "    for i_pd in range(num_pd):\n",
    "        num_reviews = len((data_jsons[i_pd])['reviews'])\n",
    "        for i_rv in range(num_reviews):\n",
    "            # tiến hành ghi review vào file tương ứng\n",
    "            stri = data_jsons[i_pd]['reviews'][i_rv]['review_text'] + '\\n'\n",
    "         \n",
    "            star = data_jsons[i_pd]['reviews'][i_rv]['review_rating']\n",
    "            if star != \"\" and stri != \"\":\n",
    "                star = float(star)\n",
    "                \n",
    "                if star <= 1:\n",
    "                    f_neg.write(stri)\n",
    "                elif star <= 3:\n",
    "                    f_neur.write(stri)\n",
    "                else:\n",
    "                    f_pos.write(stri)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            \n",
    "    \n",
    "    # đóng file text\n",
    "    f_pos.close()\n",
    "    f_neg.close()\n",
    "    f_neur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file có số dòng lớn hơn 1500\n",
    "# chia tập test và tập train\n",
    "def devideTestTrain(inputName, outputTrain, outputTest):\n",
    "    f = open(inputName, 'r', encoding=\"utf-8\")\n",
    "    ft = open(outputTest, 'w', encoding=\"utf-8\")\n",
    "    ftr = open(outputTrain, 'w', encoding=\"utf-8\")\n",
    "    f_data = f.readlines()\n",
    "    # 1200 dòng đầu dùng cho train\n",
    "    for i in range(0,1200):\n",
    "        ftr.write(f_data[i]);\n",
    "    # 500 dòng sau là cho test \n",
    "    for i in range(1200,1700):\n",
    "        ft.write(f_data[i])\n",
    "    f.close()\n",
    "    ft.close()\n",
    "    ftr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Phân chia dữ liệu\n",
    "\n",
    "cấu trúc file:\n",
    "\n",
    "* preData:\n",
    "\n",
    "> pos_data.txt\n",
    "\n",
    "> neg_data.txt\n",
    "\n",
    "> neur_data.txt\n",
    "\n",
    "---\n",
    "\n",
    "* train\n",
    "\n",
    "> pos_data: 1200\n",
    "\n",
    "> neg_data: 1200\n",
    "\n",
    "> neur_data: 1200\n",
    "\n",
    "> total : 3600\n",
    "\n",
    "* test\n",
    "\n",
    "> pos_data: 500\n",
    "\n",
    "> neg_data: 500\n",
    "\n",
    "> neur_data: 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xem kích thước của dữ liệu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tổng số review: 20936\n"
     ]
    }
   ],
   "source": [
    "def plotData(inputJson):\n",
    "    # load dữ liệu Json\n",
    "    fJson = open(inputJson, 'r')\n",
    "    dataJson = json.loads(fJson.read())\n",
    "    fJson.close()\n",
    "    start = {\n",
    "        'negative':0,\n",
    "        'neural':0,\n",
    "        'positive':0\n",
    "    }\n",
    "    sum_reviews = 0\n",
    "    #chép dũ liệu\n",
    "    for pd in dataJson:\n",
    "        sum_reviews += pd['reviews count']\n",
    "        for rv in pd['reviews']:\n",
    "            if rv['review_rating'] == 1 or rv['review_rating'] == 0 :\n",
    "                start['negative'] += 1\n",
    "            elif rv['review_rating'] == 2 or rv['review_rating'] == 3:\n",
    "                start['neural'] += 1\n",
    "            else:\n",
    "                start['positive'] += 1\n",
    "    #tiến hành vẽ\n",
    "    plt.bar(start.keys(), start.values(), align='center')\n",
    "    plt.show()\n",
    "    print(\"tổng số review: \" + str(sum_reviews))\n",
    "\n",
    "plotData('DulieuDaQuaTienXuLi.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đọc các file dữ liệu và tóm tắt dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(pos, neur, neg, num):\n",
    "    # đọc file\n",
    "    pos_df = pd.read_csv(pos, names=['text'],encoding=\"utf-8\");\n",
    "    neur_df = pd.read_csv(neur, names=['text'],encoding=\"utf-8\");\n",
    "    neg_df = pd.read_csv(neg, names=['text'],encoding=\"utf-8\");\n",
    "    # gắn nhãn\n",
    "    pos_df['label'] = 1;\n",
    "    neur_df['label'] = 0;\n",
    "    neg_df['label'] = -1;\n",
    "    \n",
    "    a = pos_df[:num]\n",
    "    b = neur_df[:num]\n",
    "    c = neg_df[:num]\n",
    "    total = a.append(b)\n",
    "    total = total.append(c)\n",
    "    \n",
    "    # tóm tắt dữ liệu\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Xây dựng model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svm = Pipeline([\n",
    "('bow', CountVectorizer()),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('classifier', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "giải thích: \n",
    "kernel:\n",
    "\n",
    "> linear: K(X,Y)=XTY\n",
    "\n",
    "> poly: K(X,Y)=(γ⋅XTY+r)d,γ>0\n",
    "\n",
    "> RBF: default: K(X,Y)=exp(∥X−Y∥2/2σ2\n",
    "\n",
    "> Sigmoid: K(X,Y)=tanh(γ⋅XTY+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SVM_model(pipeline):\n",
    "    param_svm = [{'classifier__kernel': ['linear'], 'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}]\n",
    "    return GridSearchCV(\n",
    "        pipeline, #fit dữ liệu\n",
    "        param_grid=param_svm, \n",
    "        refit=True, \n",
    "        n_jobs=-1,  # chạy trên CPUs\n",
    "        scoring='accuracy',\n",
    "        cv=StratifiedKFold(label_train, n_folds=5)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MLP_model(n_node):\n",
    "    return MLPClassifier(hidden_layer_sizes=(n_node), activation=\"tanh\",\n",
    "                                solver=\"lbfgs\", max_iter=1000, random_state=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biến đổi thành dữ liệu số"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData(data_string):\n",
    "    return CountVectorizer().fit_transform(data_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData2(data_string):\n",
    "    return TfidfVectorizer().fit_transform(data_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Huấn luyện model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Huấn luyện model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model1, text_train, label_train):\n",
    "    model1.fit(text_train, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 5) Đánh giá model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Độ lỗi trong tập huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "np.mean(label_train != grid_svm.predict(text_train))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Độ lỗi ngoài tập huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "np.mean(label_test != grid_svm.predict(text_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(classifier.predict([\"great product wife happy fit really good recommend everyone thank\"])[0])\n",
    "print(classifier.predict([\"I love this product\"])[0])\n",
    "print(classifier.predict([\"everything say reason give 45 look high quality harden rubber rugged seem still price enough protection keep stylish\"])[0])\n",
    "print(classifier.predict([\"honestly think great band like wide secure well use couple months though irritate skin like crazy think maybe user error try everything something material cause otherwise would give star stop usenow irritation get much\"])[0])\n",
    "print(classifier.predict([\"use two month one band would stay lock\"]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 3331)\n",
      "(1498, 3331)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    ### crawl Data\n",
    "    ReadAsin('DuLieuTho.json')\n",
    "    \n",
    "    ### xử lí dữ liệu\n",
    "    # chia dữ liệu khi còn là dữ liệu thô\n",
    "    # chia dựa vào số sao\n",
    "    devideData('DuLieuTho.json', 'pos_reviews.txt', 'neg_reviews.txt', 'neutr_reviews.txt')\n",
    "    # tiếp tục chia dữ liệu thành tập test và tập train\n",
    "    devideTestTrain('neg_reviews.txt', 'neg_train.txt', 'neg_test.txt')\n",
    "    devideTestTrain('neutr_reviews.txt', 'neutr_train.txt', 'neutr_test.txt')\n",
    "    devideTestTrain('pos_reviews.txt', 'pos_train.txt', 'pos_test.txt')\n",
    "    '''\n",
    "    '''\n",
    "    # tiến hành tiền xử lí cho dữ liệu train (gồm cả xử lí tay)\n",
    "    preProcData('pos_train.txt', 'pos_train_preproc.txt')\n",
    "    preProcData('neutr_train.txt', 'neutr_train_preproc.txt')\n",
    "    preProcData('neg_train.txt', 'neg_train_preproc.txt')\n",
    "    \n",
    "    # tiến hành tiền xử lí cho dữ liệu test (không xử lí tay, đảm bảo sự chính xác của model)\n",
    "    preProcData('pos_test.txt', 'pos_test_preproc.txt')\n",
    "    preProcData('neutr_test.txt', 'neutr_test_preproc.txt')\n",
    "    preProcData('neg_test.txt', 'neg_test_preproc.txt')\n",
    "    '''\n",
    "    \n",
    "    ### load dữ liệu train đã qua xử lí\n",
    "    total_data_train = readData('pos_train_preproc.txt', 'neutr_train_preproc.txt', 'neg_train_preproc.txt', 1200).reset_index()\n",
    "    ### load dữ liệu test đã qua xử lí\n",
    "    total_data_test = readData('pos_test_preproc.txt', 'neutr_test_preproc.txt', 'neg_test_preproc.txt', 500).reset_index()\n",
    "\n",
    "    # Load dữ liệu train và test đã phân chia và xử lí ở trên\n",
    "    #text_train, text_test, label_train, label_test = train_test_split(total_data['text'], total_data['label'], test_size = 0.2)\n",
    "    text_train = total_data_train['text']\n",
    "    label_train = total_data_train['label']\n",
    "    text_test = total_data_test['text']\n",
    "    label_test = total_data_test['label']\n",
    "\n",
    "    # chuyển dổi dữ liệu\n",
    "    tfidf_trans = TfidfVectorizer()\n",
    "    tfidf_text_train = tfidf_trans.fit_transform(text_train)\n",
    "    tfidf_text_test = tfidf_trans.transform(text_test)\n",
    "    print(tfidf_text_train.shape)\n",
    "    print(tfidf_text_test.shape)\n",
    "    \n",
    "    pipeline_svm = Pipeline([\n",
    "    #('bow', CountVectorizer()),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    ('classifier', SVC())\n",
    "    ])\n",
    "    # Gọi model với pipeline 1\n",
    "    grid_svm = get_SVM_model(pipeline_svm)\n",
    "\n",
    "    # huấn luyện model 1\n",
    "    train_model(grid_svm, tfidf_text_train, label_train)\n",
    "\n",
    "    # gọi model\n",
    "    # model MLP với 2 lớp ẩn và làn lượt có 5 và 5 nơ ron ẩn\n",
    "    \n",
    "    mlp = get_MLP_model([5, 5])\n",
    "    # huấn luyện\n",
    "    train_model(mlp, tfidf_text_train,label_train)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: SVM\n",
      "\tDo loi tren tap huan luyen: 11.13888888888889\n",
      "\tDo loi ngoai tap huan luyen: 27.169559412550065\n",
      "Model 2: MLP\n",
      "\tDo loi tren tap huan luyen: 0.5277777777777778\n",
      "\tDo loi ngoai tap huan luyen: 35.24699599465955\n"
     ]
    }
   ],
   "source": [
    "# đánh giá model\n",
    "# đánh giá độ lỗi trên tập huấn luyện\n",
    "print(\"Model 1: SVM\")\n",
    "print(\"\\tDo loi tren tap huan luyen: \" + str(100*np.mean(label_train != grid_svm.predict(tfidf_text_train))))\n",
    "#print(100*np.mean(label_train != grid_svm.predict(tfidf_text_train)))\n",
    "\n",
    "# Đánh giá độ lỗi ngoài tập huấn luyện\n",
    "print(\"\\tDo loi ngoai tap huan luyen: \" + str(100*np.mean(label_test != grid_svm.predict(tfidf_text_test))))\n",
    "#print(100*np.mean(label_test != grid_svm.predict(tfidf_text_test)))\n",
    "\n",
    "# đánh giá độ lỗi trên tập huấn luyện\n",
    "print(\"Model 2: MLP\")\n",
    "print(\"\\tDo loi tren tap huan luyen: \" + str(100*np.mean(label_train != mlp.predict(tfidf_text_train))))\n",
    "#print(100*np.mean(label_train != mlp.predict(tfidf_text_train)))\n",
    "\n",
    "# Đánh giá độ lỗi ngoài tập huấn luyện\n",
    "print(\"\\tDo loi ngoai tap huan luyen: \" + str(100*np.mean(label_test != mlp.predict(tfidf_text_test))))\n",
    "#print(100*np.mean(label_test != mlp.predict(tfidf_text_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thực hiện cross validation với 5-fold với kernel là SVC(kernel='linear', C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "k_accuracy = []\n",
    "for train_index, test_index in kf.split(tfidf_text_train, label_train):\n",
    "    \n",
    "    model = SVC(kernel='linear', C=1)\n",
    "    model.fit(tfidf_text_train[train_index], label_train[train_index])\n",
    "\n",
    "    score = model.score(tfidf_text_train[test_index], label_train[test_index])\n",
    "    k_accuracy.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Kết quả**\n",
      "Độ chính xác của mỗi fold:\n",
      "[0.6236111111111111, 0.6944444444444444, 0.7388888888888889, 0.7472222222222222, 0.7111111111111111]\n",
      "\n",
      "Độ chính xác trung bình:\n",
      "0.7030555555555555\n"
     ]
    }
   ],
   "source": [
    "print(\"**Kết quả**\")\n",
    "print(\"Độ chính xác của mỗi fold:\")\n",
    "print(k_accuracy)\n",
    "print(\"\\nĐộ chính xác trung bình:\")\n",
    "print(np.mean(k_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xem thông số\n",
    "\n",
    "print(grid_svm.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "tfidf_text_test = tfidf_trans.transform([\"too bab\"])\n",
    "print(grid_svm.predict(tfidf_text_test))\n",
    "#print(grid_svm.predict([\"honestly think great band like wide secure well use couple months though irritate skin like crazy think maybe user error try everything something material cause otherwise would give star stop usenow irritation get much\"]))\n",
    "#print(grid_svm.predict([\"I love this product\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả với kernel là linear:\n",
    "<img src=\".\\resultPicture\\linear.png\" width = \"200\" height = \"200\">\n",
    "\n",
    "Kết quả với kernel là sigmoid:\n",
    "<img src=\".\\resultPicture\\sigmoid.png\" width = \"200\" height = \"200\">\n",
    "\n",
    "Kết quả với kernel là rbf:\n",
    "<img src=\".\\resultPicture\\rbf.png\" width = \"200\" height = \"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preProcData('HomnayTest.txt', 'HomnayTestPreproc.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Trường hợp dự đoán sai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMuc tiêu [0]\\nDự đoán [-1]\\n\\nSư xuất hiện của từ call hay tell, không rõ nguyên nhân!\\nCó lẽ là vì trong tập dữ liệu negative xuất hiện nhiều từ call hoặc tell, dẫn đến từ này bị đánh trọng số \\ncao cho phần negative\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TH_1 = [\"I do not like this product!\"]\n",
    "tfidf_text_test = tfidf_trans.transform(TH_1)\n",
    "print(grid_svm.predict(tfidf_text_test))\n",
    "'''\n",
    "ở trương hợp 1 này sai là vì dont được giữ lại, nhưng do not thì bị bỏ not\n",
    "dẫn đến câu bị đánh vào neutral[0] thay vì negative[-1]\n",
    "'''\n",
    "\n",
    "\n",
    "TH_2 = [\"I like apple watches but i dont like this band!\"]\n",
    "tfidf_text_test = tfidf_trans.transform(TH_2)\n",
    "print(grid_svm.predict(tfidf_text_test))\n",
    "'''\n",
    "Mục tiêu [-1]\n",
    "Dự đoán [0]\n",
    "\n",
    "vì ở đây không phân tích câu theo entity mà chỉ phân tích toàn bộ review, nên khi sự xuất hiện của cả  \n",
    "dont và like trong câu làm cho câu bị nhận dạng sang neutral[0]\n",
    "'''\n",
    "\n",
    "TH_3 = [\"I dont know why but i like this!\"]\n",
    "tfidf_text_test = tfidf_trans.transform(TH_3)\n",
    "print(grid_svm.predict(tfidf_text_test))\n",
    "\n",
    "TH_4 = [\"it like another band, nothing special\"]\n",
    "tfidf_text_test = tfidf_trans.transform(TH_4)\n",
    "print(grid_svm.predict(tfidf_text_test))\n",
    "'''\n",
    "Mục tiêu [0]\n",
    "Dự đoán [1]\n",
    "Trong trường hợp này, nghĩa từ like không còn là thích nữa. Dẫn đến dự đoán sai.\n",
    "'''\n",
    "\n",
    "TH_5 = [\"tell me if you like!\"]\n",
    "tfidf_text_test = tfidf_trans.transform(TH_5)\n",
    "print(grid_svm.predict(tfidf_text_test))\n",
    "'''\n",
    "Muc tiêu [0]\n",
    "Dự đoán [-1]\n",
    "\n",
    "Sư xuất hiện của từ call hay tell, không rõ nguyên nhân!\n",
    "Có lẽ là vì trong tập dữ liệu negative xuất hiện nhiều từ call hoặc tell, dẫn đến từ này bị đánh trọng số \n",
    "cao cho phần negative\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
