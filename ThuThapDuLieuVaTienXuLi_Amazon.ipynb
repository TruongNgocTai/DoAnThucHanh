{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dòng sản phẩm: dây đeo thay thế dùng cho 'Apple watch kích thước 38mm/42mm': \n",
    "<img src=\"apple_42_38.jpg\">\n",
    "\n",
    "\n",
    "\n",
    "Danh sách các sản phẩm:\n",
    "- [Asin: B07CL119KV] iGK Sport Band Compatible for Apple Watch 42mm 38mm, Soft Silicone Sport Strap Replacement Bands Compatible for iWatch Apple Watch Series 3, Series 2, Series 1 S/M M/L <img src=\".\\productPicture\\iGK.png\">\n",
    "- [Asin: B077VLBBYT] Yunsea Compatible for Apple Watch Band 38mm 42mm, Soft Nylon Sport Loop, with Hook and Loop Fastener, Replacement Band Compatible for iWatch Series 1/2/3 <img src=\".\\productPicture\\yunsea.png\">\n",
    "- [Asin: B0786QKGX3] VATI Replacement Band Compatible for Apple Watch Band 38mm 42mm Soft Breathable Nylon Sport Loop Band Adjustable Wrist Strap Replacement Band Compatible for iWatch Series 3/2/1,Sport,Nike+,Edition <img src=\".\\productPicture\\vati.png\">\n",
    "- [Asin: B07BQWGVDX] OULEDI Compatible Stainless Steel Band for Apple Watch Replacement Mesh Strap Bracelet for iWatch Series 1 Series 2 Series 3 Series 4 with Magnetic Closure Clasp 38mm 40mm Rose Gold <img src=\".\\productPicture\\ouledi.png\">\n",
    "- [Asin: B07GTG8FM3] iYou Sport Band Compatible for Apple Watch Band 38MM 42MM, Soft Silicone Replacement Sport Strap Compatible for iWatch 2017 Apple Watch Series 3/2/1, Edition, Nike+, All Models More Colors Choose <img src=\".\\productPicture\\iyou.png\">\n",
    "- [Asin: B075R4NNPH] Waterproof Apple Watch Case 38mm Series 3 & 2 with Premium Soft Silicone Apple Watch Band by Catalyst, Shock Proof Impact Resistant (not Compatible with The 42mm iWatch) <img src=\".\\productPicture\\Waterproof.png\">\n",
    "- [Asin: B079NJY3QL] Catalyst Apple Watch Case 38mm Series 3 & Series 2 Drop Proof Shock Proof Impact Protection Apple Watch case [Rugged iWatch Protective case], Army Green <img src=\".\\productPicture\\Catalyst.png\">\n",
    "- [Asin: B077NCMCR6] For Apple Watch Band 38mm, Maxjoy Nylon iWatch Strap Replacement Bands with Stainless Metal Clasp for Apple Watch Series 3 Series 2 Series 1 Sport and Edition, Army Green <img src=\".\\productPicture\\Maxjoy.png\">\n",
    "- [Asin: B071FK7GS6] Compatible Apple Watch Band 38mm Case, Camyse Shockproof Rugged Protective Cover with Bands Stainless Steel Clasp for iWatch Apple Watch Series 3, 2, 1 Sport Edition for Men Women grils boys - Black <img src=\".\\productPicture\\Camyse.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# các thư viện sử dụng\n",
    "from lxml import html  \n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import inflect\n",
    "\n",
    "#vẽ dữ liệu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dateutil import parser as dateparser\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#remove stop word\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "# stemming\n",
    "# PorterStemmer để steming\n",
    "# WordNetLematizer để chuyển đổi từ loại\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# tranform vector su dugn tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# model training\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# download data of nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('sentiwordnet')\n",
    "## n - NOUN \n",
    "## v - VERB \n",
    "## a - ADJECTIVE \n",
    "## s - ADJECTIVE SATELLITE \n",
    "## r - ADVERB \n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseReviews(asin):\n",
    "    #This script has only been tested with Amazon.com\n",
    "    amazon_url  = 'http://www.amazon.com/product-reviews/'+asin\n",
    "    # Add some recent user agent to prevent amazon from blocking the request \n",
    "    # Find some chrome user agent strings  here https://udger.com/resources/ua-list/browser-detail?browser=Chrome\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'}\n",
    "\n",
    "    for i in range(3):\n",
    "\n",
    "        response = requests.get(amazon_url,headers = headers,verify=False)\n",
    "        if response.status_code==404:\n",
    "            return {\"url\":amazon_url,\"error\":\"page not found\"}\n",
    "        if response.status_code!=200:\n",
    "            continue\n",
    "\n",
    "        '''\n",
    "        The HTML xpath parser from lxml.etree seems to have max depth limit. \n",
    "        It won't traverse further to parse the text if the depth exceeds 254. \n",
    "        To avoid this, splitting the html into chunks\n",
    "        '''\n",
    "        # chuyển thành cây\n",
    "        tree = html.fromstring(response.text)\n",
    "        # gán xpath\n",
    "        xpath_pageNext = '//li[@data-reftag=\"cm_cr_arp_d_paging_btm\"]'\n",
    "\n",
    "        # tiến hành lấy thông tin link các trang\n",
    "        ls_page = tree.xpath(xpath_pageNext)\n",
    "        \n",
    "        ## lấy được các trang rồi thì break ngắt vong lập\n",
    "        break\n",
    "    \n",
    "    # lấy tất cả các link trang chứa reviews\n",
    "    # links\n",
    "    links = []\n",
    "    # lấ số lượng trang reviews\n",
    "    s = ls_page[4].find('a').get('href')\n",
    "    num_page = int(ls_page[4].find('a').get('href').split('&pageNumber=')[1])\n",
    "    for i in range(2, num_page + 1):\n",
    "        url = 'https://amazon.com/' + re.sub(r'ref=cm_cr_arp_d_paging_btm_.*?ie=UTF8&pageNumber=.*$', 'ref=cm_cr_arp_d_paging_btm_' + str(i) + '?ie=UTF8&pageNumber=' +  str(i), s)\n",
    "        links.append(url)\n",
    "    \n",
    "    # lấy reviews ở trang hiện tại\n",
    "    xpath_reviewsList = '//div[@id=\"cm_cr-review_list\"]'\n",
    "    xpath_review = '//div[@data-hook=\"review\"]'\n",
    "    \n",
    "    review_str = tree.xpath(xpath_reviewsList)\n",
    "    \n",
    "    for i in review_str:\n",
    "        # lấy từng chuỗi review\n",
    "        reviews = i.xpath(xpath_review)\n",
    "    \n",
    "    ## cấu trúc lưu dữ liệu là 1 dic.\n",
    "    reviews_list = []\n",
    "    \n",
    "    ## parse từng chuỗi review:\n",
    "    for review in reviews:\n",
    "        \n",
    "        ls_div = review.find('div').findall('div')\n",
    "        #### chỉ lấy 4 div đầu tiên theo thứ tự bên dưới\n",
    "        ### lấy tên\n",
    "        if ls_div[0].find('a') is None:\n",
    "                continue\n",
    "        name = ls_div[0].find('a').findall('div', {'class': 'a-profile-content'})#.findall('div')\n",
    "        name1 = name[1].find('span', {'class': 'a-profile-avatar-wrapper'}).text\n",
    "        \n",
    "        ### đầu tiên là lấy số sao\n",
    "        star = ls_div[1].find('a').find('i').find('span').text.replace(' out of 5 stars', '')\n",
    "        ### tiếp theo lấy thời gian\n",
    "        date = review.find('div').find('span').text\n",
    "        ### tiếp theo lấy text\n",
    "        review_content = ls_div[3].find('span').text\n",
    "        \n",
    "        ### tiến hành lưu dữ liệu ### tất cả review được chuyển về lower case\n",
    "        if review_content is not None:\n",
    "            dic_review = {\n",
    "                'review_author' : name1,\n",
    "                'review_rating' : star,\n",
    "                'review_posted_date' : date,\n",
    "                'review_text' : review_content.lower()\n",
    "            }\n",
    "            reviews_list.append(dic_review)\n",
    "        \n",
    "\n",
    "    # lấy reviews trong các trang còn lại\n",
    "    for link in links:\n",
    "        response = requests.get(link,headers = headers,verify=False)\n",
    "        \n",
    "        # chuyển thành cây\n",
    "        tree = html.fromstring(response.text)\n",
    "        \n",
    "        # tiến hành lấy dữ liệu như trên\n",
    "        review_str = tree.xpath(xpath_reviewsList)\n",
    "    \n",
    "        for i in review_str:\n",
    "            # lấy từng chuỗi review\n",
    "            reviews = i.xpath(xpath_review)\n",
    "        \n",
    "        ## parse từng chuỗi review:\n",
    "        for review in reviews:\n",
    "\n",
    "            ls_div = review.find('div').findall('div')\n",
    "            #### chỉ lấy 4 div đầu tiên theo thứ tự bên dưới\n",
    "            ### lấy tên\n",
    "            if ls_div[0].find('a') is None:\n",
    "                continue\n",
    "            name = ls_div[0].find('a').findall('div', {'class': 'a-profile-content'})#.findall('div')\n",
    "            name1 = name[1].find('span', {'class': 'a-profile-avatar-wrapper'}).text\n",
    "\n",
    "            ### đầu tiên là lấy số sao\n",
    "            star = ls_div[1].find('a').find('i').find('span').text.replace(' out of 5 stars', '')\n",
    "            ### tiếp theo lấy thời gian\n",
    "            date = review.find('div').find('span').text\n",
    "            ### tiếp theo lấy text\n",
    "            review_content = ls_div[3].find('span').text\n",
    "            \n",
    "            ### tiến hành lưu dữ liệu ### tất cả review được chuyển về lower case\n",
    "            if review_content is not None:\n",
    "                dic_review = {\n",
    "                    'review_author' : name1,\n",
    "                    'review_rating' : star,\n",
    "                    'review_posted_date' : date,\n",
    "                    'review_text' : review_content.lower()\n",
    "                }\n",
    "                reviews_list.append(dic_review)\n",
    "            \n",
    "    return reviews_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadAsin(input):\n",
    "    AsinList = ['B01MS9O4JC', 'B015WKAQ1K', 'B07C44LMVQ', 'B07CSYNZMG', 'B07C68N8F4', 'B018ONN290', 'B01B61NLU2', 'B01NAWBGQH', 'B01AIH23QS', 'B01M2YQ73M', 'B07B9SLGLK', 'B075R4NNPH', 'B077NCMCR6', 'B071FK7GS6', 'B07CL119KV', 'B077VLBBYT', 'B0786QKGX3', 'B07BQWGVDX', 'B07C44LMVP']\n",
    "    extracted_data = []\n",
    "    for asin in AsinList:\n",
    "        print(\"Downloading and processing page http://www.amazon.com/product-reviews/\"+asin)\n",
    "        reviews = ParseReviews(asin)\n",
    "        reviews_len = len(reviews)\n",
    "        dic = {\n",
    "            'asin' : asin,\n",
    "            'reviews count' : reviews_len,\n",
    "            'reviews' : reviews\n",
    "        }\n",
    "        extracted_data.append(dic)\n",
    "        sleep(5)\n",
    "    with open(input, \"w\", encoding=\"utf-8\") as f:\n",
    "    #   f.write(extracted_data)\n",
    "    #f = open('DuLieuTho.json','w')\n",
    "        json.dump(extracted_data,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đọc dữ liệu lên và tiền xử lí\n",
    "\n",
    "loại bỏ stopwords\n",
    "\n",
    "ngắt câu\n",
    "\n",
    "---\n",
    "\n",
    "file input: blabla_data.json\n",
    "\n",
    "file output: preProcData.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(review):\n",
    "    # khởi tạo 1 set stop word\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    # tách câu thành tokens  \n",
    "    word_tokens = word_tokenize(review)\n",
    "    filtered_sentence = word_tokens\n",
    "    # tra từng tokens trong set stopword và lưu phần còn lại vào lại câu\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    # tiến hành stemming từng từ trong câu\n",
    "    #filtered_sentence = [(WordNetLemmatizer().lemmatize(filtered_sentence[i],pos = 'v') if WordNetLemmatizer().lemmatize(filtered_sentence[i]).endswith('e' or 's' or 'ss' or 'sess') else PorterStemmer().stem(filtered_sentence[i])) for i in range(len(filtered_sentence))]\n",
    "    #filtered_sentence = [PorterStemmer().stem(filtered_sentence[i]) for i in range(len(filtered_sentence))]\n",
    "    filtered_sentence = [WordNetLemmatizer().lemmatize(filtered_sentence[i],pos = 'v') for i in range(len(filtered_sentence))]\n",
    "    # tiến hành convert số thành chữ tương ứng với số\n",
    "    filtered_sentence = [(inflect.engine().number_to_words(filtered_sentence[i]) if filtered_sentence[i].isdigit() else filtered_sentence[i]) for i in range(len(filtered_sentence))]\n",
    "    # trả về\n",
    "    re_review = ' '.join(filtered_sentence)\n",
    "    return re_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcReviews( input, outputJson):\n",
    "    # đọc file và load dữ liệu\n",
    "    f = open(input,'r')\n",
    "    data_jsons = json.loads(f.read())\n",
    "    f.close()\n",
    "    \n",
    "    num_pd = len(data_jsons)\n",
    "    for i_pd in range(num_pd):\n",
    "        num_reviews = len((data_jsons[i_pd])['reviews'])\n",
    "        for i_rv in range(num_reviews):\n",
    "            # chuyển start thành số\n",
    "            data_jsons[i_pd]['reviews'][i_rv]['review_rating'] = float(data_jsons[i_pd]['reviews'][i_rv]['review_rating'])\n",
    "            '''\n",
    "            # tiến hành tách câu sử dụng pickle của nltk\n",
    "            data_jsons[i_pd]['reviews'][i_rv]['review_text'] = tokenizer.tokenize(data_jsons[i_pd]['reviews'][i_rv]['review_text'], realign_boundaries=True)\n",
    "            #tiến hành tách từ và loại bỏ stopwords trong mỗi câu đồng thời stemming trong từng câu\n",
    "            num_sentences = len(data_jsons[i_pd]['reviews'][i_rv]['review_text'])\n",
    "            for sentence in range(num_sentences):\n",
    "                data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence] = removeStopWords(data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence])\n",
    "                # tiến hành loại bỏ dấu câu\n",
    "                data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence] = re.sub(r'[^a-zA-Z0-9 ]',r'',data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence])\n",
    "                # tiến hành loại bỏ tất cả các khoảng trắng dư thừa trong câu\n",
    "                data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence] = (re.sub(' +', ' ', data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence])).strip()\n",
    "                raw = ''.join(data_jsons[i_pd]['reviews'][i_rv]['review_text'][sentence])\n",
    "            '''\n",
    "            # remove stop word\n",
    "            data_jsons[i_pd]['reviews'][i_rv]['review_text'] = removeStopWords(data_jsons[i_pd]['reviews'][i_rv]['review_text'])\n",
    "            # loại bỏ dấu câu\n",
    "            data_jsons[i_pd]['reviews'][i_rv]['review_text'] = re.sub(r'[^a-zA-Z0-9 ]',r'',data_jsons[i_pd]['reviews'][i_rv]['review_text'])\n",
    "            #loại bỏ khoảng trắng dư thừa\n",
    "            data_jsons[i_pd]['reviews'][i_rv]['review_text'] = (re.sub(' +', ' ', data_jsons[i_pd]['reviews'][i_rv]['review_text'])).strip()\n",
    "            \n",
    "            \n",
    "    # tiến hành lưu lại dữ liệu\n",
    "    with open(outputJson, \"w\", encoding=\"utf-8\") as fJson:\n",
    "    #    f.write(data_jsons)\n",
    "    #f = open(output,'w')\n",
    "        json.dump(data_jsons,fJson,indent=4)\n",
    "        \n",
    "    print('Done!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xử lí tay:\n",
    "\n",
    "bỏ các kí tự: s, ve, ca, t, 38mm, 42mm, d.\n",
    "\n",
    "thay thế: don = dont, doesn = doesnt, didn = didnt, isn = inst, aren = arent, 2nd, 3rd, 6th, ...\n",
    "\n",
    "chỉnh sữa: soooo.., toooooo, \n",
    "\n",
    "bỏ câu tiếng bla bla @@: buenas noches este artculo ha llegado mis manos enviaron una cosa por otra, pierde el brillo de inmediato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devideData (inputJson, pos_txt, neg_txt, neur_txt):\n",
    "    # đọc file và load dữ liệu\n",
    "    f = open(inputJson,'r')\n",
    "    data_jsons = json.loads(f.read())\n",
    "    f.close()\n",
    "    \n",
    "    # mở file text\n",
    "    f_pos = open(pos_txt, 'w')\n",
    "    f_neg = open(neg_txt, 'w')\n",
    "    f_neur = open(neur_txt, 'w')\n",
    "    \n",
    "    num_pd = len(data_jsons)\n",
    "    for i_pd in range(num_pd):\n",
    "        num_reviews = len((data_jsons[i_pd])['reviews'])\n",
    "        for i_rv in range(num_reviews):\n",
    "            # tiến hành ghi review vào file tương ứng\n",
    "            stri = data_jsons[i_pd]['reviews'][i_rv]['review_text'] + '\\n'\n",
    "            star = data_jsons[i_pd]['reviews'][i_rv]['review_rating']\n",
    "\n",
    "            if( star <= 1):\n",
    "                f_neg.write(stri)\n",
    "            elif(star <= 3):\n",
    "                f_neur.write(stri)\n",
    "            else:\n",
    "                f_pos.write(stri)\n",
    "    \n",
    "    # đóng file text\n",
    "    f_pos.close()\n",
    "    f_neg.close()\n",
    "    f_neur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lưu dữ liệu\n",
    "\n",
    "Dữ liệu: \n",
    "\n",
    "X : tập các reviews\n",
    "\n",
    "y : nhãn ứng với các reviews trong X\n",
    "\n",
    "\n",
    "tiến hành chia dữ liệu\n",
    "\n",
    "sklearn.model_selection.train_test_split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xem phân bố của dữ liệu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tổng số review: 20936\n"
     ]
    }
   ],
   "source": [
    "def plotData(inputJson):\n",
    "    # load du lieuj Json\n",
    "    fJson = open(inputJson, 'r')\n",
    "    dataJson = json.loads(fJson.read())\n",
    "    fJson.close()\n",
    "    start = {\n",
    "        'negative':0,\n",
    "        'neural':0,\n",
    "        'positive':0\n",
    "    }\n",
    "    sum_reviews = 0\n",
    "    #chép dũ liệu\n",
    "    for pd in dataJson:\n",
    "        sum_reviews += pd['reviews count']\n",
    "        for rv in pd['reviews']:\n",
    "            if rv['review_rating'] == 1 or rv['review_rating'] == 0 :\n",
    "                start['negative'] += 1\n",
    "            elif rv['review_rating'] == 2 or rv['review_rating'] == 3:\n",
    "                start['neural'] += 1\n",
    "            else:\n",
    "                start['positive'] += 1\n",
    "    #tiến hành vẽ\n",
    "    plt.bar(start.keys(), start.values(), align='center')\n",
    "    plt.show()\n",
    "    print(\"tổng số review: \" + str(sum_reviews))\n",
    "\n",
    "plotData('DulieuDaQuaTienXuLi.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phân chia dữ liệu\n",
    "\n",
    "cấu trúc file:\n",
    "\n",
    "+preData:\n",
    "\n",
    "+X\n",
    "\n",
    "+---pos_data.txt\n",
    "\n",
    "+---neg_data.txt\n",
    "\n",
    "+---neur_data.txt\n",
    "\n",
    "+Y\n",
    "\n",
    "+---y.txt\n",
    "\n",
    "---\n",
    "\n",
    "train : validation\n",
    "\n",
    "--80%-----20%------\n",
    "\n",
    "---\n",
    "\n",
    "+dùng\n",
    "\n",
    "+---pos_data: 1500\n",
    "\n",
    "+---neg_data: 1500\n",
    "\n",
    "+---neur_data: 1500\n",
    "\n",
    "+---total : 4500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đọc các file dữ liệu và tóm tắt dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(pos, neur, neg, num):\n",
    "    # đọc file\n",
    "    pos_df = pd.read_csv(pos, names=['text'],encoding=\"utf-8\");\n",
    "    neur_df = pd.read_csv(neur, names=['text'],encoding=\"utf-8\");\n",
    "    neg_df = pd.read_csv(neg, names=['text'],encoding=\"utf-8\");\n",
    "    # gắn nhãn\n",
    "    pos_df['label'] = 1;\n",
    "    neur_df['label'] = 0;\n",
    "    neg_df['label'] = -1;\n",
    "    \n",
    "    a = pos_df[:num]\n",
    "    b = neur_df[:num]\n",
    "    c = neg_df[:num]\n",
    "    total = a.append(b)\n",
    "    total = total.append(c)\n",
    "    \n",
    "    # tóm tắt dữ liệu\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4320 1080 4320 1080\n",
      "Wall time: 1min 56s\n",
      "[mean: 0.73611, std: 0.00969, params: {'classifier__C': 1, 'classifier__kernel': 'linear'}, mean: 0.70486, std: 0.00948, params: {'classifier__C': 10, 'classifier__kernel': 'linear'}, mean: 0.67315, std: 0.01101, params: {'classifier__C': 100, 'classifier__kernel': 'linear'}, mean: 0.65509, std: 0.01226, params: {'classifier__C': 1000, 'classifier__kernel': 'linear'}, mean: 0.33565, std: 0.00035, params: {'classifier__C': 1, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}, mean: 0.33565, std: 0.00035, params: {'classifier__C': 1, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}, mean: 0.49884, std: 0.00792, params: {'classifier__C': 10, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}, mean: 0.33565, std: 0.00035, params: {'classifier__C': 10, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}, mean: 0.73403, std: 0.01187, params: {'classifier__C': 100, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}, mean: 0.49907, std: 0.00834, params: {'classifier__C': 100, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}, mean: 0.73472, std: 0.00856, params: {'classifier__C': 1000, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}, mean: 0.73380, std: 0.01174, params: {'classifier__C': 1000, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ### crawl Data\n",
    "    #ReadAsin('DuLieuTho.json')\n",
    "    \n",
    "    ### xử lí dữ liệu\n",
    "    #preProcReviews('DuLieuTho.json', 'DulieuDaQuaTienXuLi.json')\n",
    "    #devideData('DulieuDaQuaTienXuLi.json', 'pos_reviews.txt', 'neg_reviews.txt', 'neur_reviews.txt')\n",
    "    \n",
    "    ### load dữ liệu\n",
    "    total_data = readData('pos_reviews.txt', 'neur_reviews.txt', 'neg_reviews.txt', 1800)\n",
    "    #print(total_data)\n",
    "    #print(total_data.groupby('label').describe())\n",
    "    \n",
    "    ### xây dựng model\n",
    "    #convert review thành vector \n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(total_data['text'])\n",
    "    X_train_counts.shape\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    X_train_tfidf.shape\n",
    "    \n",
    "    #phân chia dữ liệu\n",
    "    text_train, text_test, label_train, label_test = train_test_split(total_data['text'], total_data['label'], test_size = 0.2)\n",
    "    print(len(text_train), len(text_test), len(label_train), len(label_test))\n",
    "    \n",
    "    #khai báo pipeline\n",
    "    pipeline_svm = Pipeline([\n",
    "        ('bow', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('classifier', SVC()),\n",
    "    ])\n",
    "    \n",
    "    #\n",
    "    param_svm = [\n",
    "      {'classifier__C': [1, 10, 100, 1000], 'classifier__kernel': ['linear']},\n",
    "      {'classifier__C': [1, 10, 100, 1000], 'classifier__gamma': [0.001, 0.0001], 'classifier__kernel': ['rbf']},\n",
    "    ]\n",
    "    \n",
    "    #\n",
    "    grid_svm = GridSearchCV(\n",
    "        pipeline_svm, #fit dữ liệu\n",
    "        param_grid=param_svm, \n",
    "        refit=True, \n",
    "        n_jobs=-1,  # chạy trên CPUs\n",
    "        scoring='accuracy',\n",
    "        cv=StratifiedKFold(label_train, n_folds=5),\n",
    "    )\n",
    "    \n",
    "    %time classifier = grid_svm.fit(text_train, label_train)\n",
    "    print(classifier.grid_scores_)\n",
    "    \n",
    "    # report\n",
    "    \n",
    "    ## download du lieu lan 2\n",
    "    #ReadAsin('DuLieuTho1.json')\n",
    "    #preProcReviews('DuLieuTho1.json', 'DulieuDaQuaTienXuLi1.json')\n",
    "    \n",
    "    ## download du lieu lan 3\n",
    "    #ReadAsin('DuLieuTho2.json')\n",
    "    #preProcReviews('DuLieuTho2.json', 'DulieuDaQuaTienXuLi2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### predict and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.68      0.73      0.71       350\n",
      "          0       0.65      0.66      0.66       363\n",
      "          1       0.91      0.83      0.87       367\n",
      "\n",
      "avg / total       0.75      0.74      0.74      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_test, classifier.predict(text_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict([\"I love this product\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict([\"everything say reason give 45 look high quality harden rubber rugged seem still price enough protection keep stylish\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict([\"honestly think great band like wide secure well use couple months though irritate skin like crazy think maybe user error try everything something material cause otherwise would give star stop usenow irritation get much\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict([\"use two month one band would stay lock\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict([\"great product wife happy fit really good recommend everyone thank\"])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chuyển đổi review -> vector\n",
    "\n",
    "Dùng CountVectorizer().fit(review).transformer(review) -> TfidfVectorizer().fit(review)->trandformer(review) -> vector_tfidf\n",
    "\n",
    "Dùng SVM\n",
    " dùng GridSeachCV()\n",
    "\n",
    "\n",
    "Pipeline\n",
    "GridSearchCV\n",
    "StratifiedKFold, cross_val_score, train_test_split "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
